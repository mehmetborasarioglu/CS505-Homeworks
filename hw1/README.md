# SMS Character‑Level Language Model — Homework 1

## Overview

This repository contains the source code and documentation for Homework 1 of the Spring 2025 **Character‑Level Language Modeling** assignment.  The objective is to build and evaluate statistical and neural language models that predict the next character in an SMS message drawn from the English subset of the NUS SMS Corpus.

Implemented models

* **Unigram baselines** – statistical and neural implementations.
* **5‑gram language model** with absolute discounting and back‑off.
* **Recurrent neural networks (RNN) and Long Short‑Term Memory networks (LSTM)** for character prediction.
* **Interactive prediction script** for live autocompletion in the terminal.

A detailed methodology, experimental setup, and discussion of results are provided in `report.pdf`.

---

## Repository structure

| Path                      | Description                                             |
| ------------------------- | ------------------------------------------------------- |
| `baseline_statistical.py` | Statistical unigram baseline.                           |
| `baseline_nn.py`          | Neural unigram baseline.                                |
| `experiment_ngram.py`     | Training and evaluation script for 5‑gram models.       |
| `experiment_rnn.py`       | Training and evaluation script for RNN and LSTM models. |
| `predict.py`              | Interactive autocompletion demo.                        |
| `models/ngram/`           | Count‑based language‑model implementations.             |
| `models/nn/`              | Neural language‑model implementations.                  |
| `data/`                   | Training, development, and test corpora.                |
| `hw1.pdf`                 | Assignment specification.                               |
| `report.pdf`              | Technical report and results.                           |

---

## Installation

This project requires **Python ≥ 3.9**.

```bash
# (optional) create and activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate

# install dependencies
pip install -r requirements.txt
```

*Key dependencies*: NumPy, SciPy, PyTorch 2.x, and tqdm.

---

## Usage

### Baseline models

```bash
python baseline_statistical.py   # statistical unigram (~16 % dev accuracy)
python baseline_nn.py            # neural unigram (~16 % dev accuracy)
```

### 5‑gram model

```bash
python experiment_ngram.py       # trains 5‑gram, saves ngram.model
```

Expected development‑set accuracy: **≈ 50 %**.

### Neural models

```bash
python experiment_rnn.py         # trains RNN and LSTM models
```

* Vanilla RNN: development accuracy ≈ 42 %
* LSTM (best model): development accuracy ≈ 49 %

Model checkpoints (`*.model`) are cached and automatically re‑loaded on subsequent runs.

### Interactive prediction

```bash
python predict.py data/large
```

Enter text at the prompt; the script prints a 20‑character completion generated by the selected model.

---

## Results (development set)

| Model                 | Accuracy |
| --------------------- | -------- |
| Unigram (statistical) | 16 %     |
| Unigram (neural)      | 16 %     |
| 5‑gram + discounting  | 50 %     |
| RNN                   | 42 %     |
| LSTM                  | 49 %     |

Refer to `report.pdf` for full training curves, hyper‑parameter settings, and ablation studies.

---

## Customisation

* **N‑gram parameters** (order, discount value) can be modified in `experiment_ngram.py`.
* **Neural hyper‑parameters** (hidden size, number of layers, dropout, epochs) can be adjusted via command‑line arguments in `experiment_rnn.py`.
* `predict.py` accepts any trained model by file path, enabling rapid experimentation.

---

## Acknowledgements

* Assignment materials and starter code were provided by the course instructional staff.
* The SMS data are distributed under CC‑BY‑NC‑SA 4.0 via the NUS SMS Corpus.

---

## Contact

For questions or feedback, please contact **Mehmet Sarioglu** ([sarioglu@bu.edu](mailto:sarioglu@bu.edu)).
